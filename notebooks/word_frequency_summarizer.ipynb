{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUEq8nGVXtwL"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebloqnjuIbdh"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from goose3 import Goose\n",
    "from dstoolkit.nlp import WordFreqSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_resumo(titulo, lista_sentencas, melhores_sentencas):\n",
    "    from IPython.core.display import HTML\n",
    "    texto = ''\n",
    "\n",
    "    display(HTML(f'<h1>Resumo do texto - {titulo}</h1>'))\n",
    "    for i in lista_sentencas:\n",
    "        if i in melhores_sentencas:\n",
    "            texto += str(i).replace(i, f\"<mark>{i}</mark>\")\n",
    "        else:\n",
    "            texto += i\n",
    "    display(HTML(f\"\"\" {texto} \"\"\"))\n",
    "\n",
    "\n",
    "def preprocessamento(texto):\n",
    "  \n",
    "    texto_formatado = texto.lower()\n",
    "    tokens = []\n",
    "\n",
    "    for token in nltk.word_tokenize(texto_formatado):\n",
    "        tokens.append(token)\n",
    "\n",
    "    tokens = [palavra for palavra in tokens if palavra not in stopwords and palavra not in string.punctuation]\n",
    "    texto_formatado = ' '.join([str(elemento) for elemento in tokens if not elemento.isdigit()])\n",
    "\n",
    "    return texto_formatado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9qp7RfhZAaK"
   },
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UTS2OxXCZFy6"
   },
   "outputs": [],
   "source": [
    "g = Goose()\n",
    "url = 'https://iaexpert.academy/2020/11/09/ia-preve-resultado-das-eleicoes-americanas/'\n",
    "artigo = g.extract(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vokH5XcBZlfX",
    "outputId": "a4fb967f-9376-47fc-86dc-b23fe8b51053"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ccny.cuny.edu/profiles/hernan-makse',\n",
       " 'https://www.express.co.uk/news/science/1355192/artificial-intelligence-ai-news-twitter-us-election-2020-prediction-trump-biden-evg']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artigo.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4fNAkpZPSACS"
   },
   "outputs": [],
   "source": [
    "artigo.links.append('https://www.ocupacoes.com.br/cbo-mte/251205-economista&#8221')\n",
    "artigo.links.append('https://www.ocupacoes.com.br/cbo-mte/251405-filosofo&#8221')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "jkHF04VqZqot",
    "outputId": "55bfbf36-76f9-4907-e456-61a4d7e63c54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nas eleições presidenciais americanas de 2016, a maioria das predições apontavam para a vitória de Hillary Clinton. Entretanto, a história nos mostrou o resultado oposto, e Donald Trump foi o presidente nos últimos 4 anos. Desta vez, os estatísticos reexaminaram seus modelos, para aumentar o grau de confiabilidade nos seus resultados. Nesta tentativa de otimização das predições, a inteligência artificial certamente não ficou de fora.\\n\\nO modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais. O algoritmo fez a análise de cerca de 1 bilhão de tweets para chegar a uma estimativa dos resultados do pleito. No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden.\\n\\nO Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado. Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores. O trabalho não depende apenas da coleta dos dados, mas também de um tratamento estatístico adequado para levar em consideração duas variáveis externas: o viés de amostragem e a taxa de comparecimento. O primeiro fator se refere ao fato de que as redes sociais não necessariamente representam a população americana. A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual. O segundo fator se deve à não-obrigatoriedade de votação nos Estados Unidos, ou seja, por mais que uma pessoa tenha sua preferência, pode ser que ela não compareça aos locais de votação para efetivá-la. Segundo o Dr. Makse, integrar estas duas variáveis em seu modelo é a parte mais importante do trabalho. Ele acredita ser esta uma das razões para que as estimativas da última eleição, baseadas em métodos tradicionais de coleta de informação, terem falhado. Sua equipe acompanhou as tendências apresentadas nas últimas eleições na Europa, e os modelos estão se revelando cada vez melhores.\\n\\nQuando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem. Após aplicar os mecanismos de correção para os dois vieses identificados, a vantagem diminuiu, mas Biden ainda é indicado como favorito.\\n\\nParece que, desta vez, os algoritmos estão de fato contribuindo para que as predições sejam mais precisas.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artigo.cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "d5330LEeZy9E",
    "outputId": "9c874f4a-4eb1-4522-ec6a-a1667d59953b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nas eleições presidenciais americanas de 2016, a maioria das predições apontavam para a vitória de Hillary Clinton. Entretanto, a história nos mostrou o resultado oposto, e Donald Trump foi o presidente nos últimos 4 anos. Desta vez, os estatísticos reexaminaram seus modelos, para aumentar o grau de confiabilidade nos seus resultados. Nesta tentativa de otimização das predições, a inteligência artificial certamente não ficou de fora.\\n\\nO modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais. O algoritmo fez a análise de cerca de 1 bilhão de tweets para chegar a uma estimativa dos resultados do pleito. No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden.\\n\\nO Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado. Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores. O trabalho não depende apenas da coleta dos dados, mas também de um tratamento estatístico adequado para levar em consideração duas variáveis externas: o viés de amostragem e a taxa de comparecimento. O primeiro fator se refere ao fato de que as redes sociais não necessariamente representam a população americana. A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual. O segundo fator se deve à não-obrigatoriedade de votação nos Estados Unidos, ou seja, por mais que uma pessoa tenha sua preferência, pode ser que ela não compareça aos locais de votação para efetivá-la. Segundo o Dr. Makse, integrar estas duas variáveis em seu modelo é a parte mais importante do trabalho. Ele acredita ser esta uma das razões para que as estimativas da última eleição, baseadas em métodos tradicionais de coleta de informação, terem falhado. Sua equipe acompanhou as tendências apresentadas nas últimas eleições na Europa, e os modelos estão se revelando cada vez melhores.\\n\\nQuando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem. Após aplicar os mecanismos de correção para os dois vieses identificados, a vantagem diminuiu, mas Biden ainda é indicado como favorito.\\n\\nParece que, desta vez, os algoritmos estão de fato contribuindo para que as predições sejam mais precisas.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artigo_original = artigo.cleaned_text\n",
    "artigo_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "y_Py0p-tZ5o9",
    "outputId": "945b3a69-3124-4287-ef67-2e57150ee80b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eleições presidenciais americanas maioria predições apontavam vitória hillary clinton entretanto história mostrou resultado oposto donald trump presidente últimos anos desta vez estatísticos reexaminaram modelos aumentar grau confiabilidade resultados nesta tentativa otimização predições inteligência artificial certamente ficou modelo desenvolvido dr. hernan makse físico estatístico universidade cidade nova york baseou predições rede neural treinada processar sentimentos expressos redes sociais algoritmo fez análise cerca bilhão tweets chegar estimativa resultados pleito dia eleição novembro modelo indicando vitória joe biden dr. makse disse trabalho começou eleição testado novamente eleições argentina ano passado desta vez modelo treinando cerca vezes dados eleições americanas anteriores trabalho depende apenas coleta dados tratamento estatístico adequado levar consideração duas variáveis externas viés amostragem taxa comparecimento primeiro fator refere fato redes sociais necessariamente representam população americana participação redes sociais costuma maior cidades grandes fato têm preferência candidatos modelo deve corrigido levar consideração opinião pessoas ativas neste ambiente virtual segundo fator deve não-obrigatoriedade votação estados unidos pessoa preferência pode compareça locais votação efetivá-la segundo dr. makse integrar duas variáveis modelo parte importante trabalho acredita razões estimativas última eleição baseadas métodos tradicionais coleta informação terem falhado equipe acompanhou tendências apresentadas últimas eleições europa modelos revelando cada vez melhores modelo usado predizer resultados eleição corrente usando dados brutos joe biden apareceu vencedor larga vantagem após aplicar mecanismos correção dois vieses identificados vantagem diminuiu biden ainda indicado favorito parece desta vez algoritmos fato contribuindo predições precisas'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artigo_formatado = preprocessamento(artigo_original)\n",
    "artigo_formatado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Qz2wttDaGvw",
    "outputId": "5658ee67-2b43-40d6-b5c0-8007b74267c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1901"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(artigo_formatado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = WordFreqSummarizer(n_sentences=5, lemma=False, spacy_model=\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VJ2u4sRlbMu_"
   },
   "outputs": [],
   "source": [
    "best_sentences = s.summarize(artigo_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual.',\n",
       " 'O modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais.',\n",
       " 'Quando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem.',\n",
       " 'O trabalho não depende apenas da coleta dos dados, mas também de um tratamento estatístico adequado para levar em consideração duas variáveis externas: o viés de amostragem e a taxa de comparecimento.',\n",
       " 'Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "6WteYBEhdIqV",
    "outputId": "5a608627-494b-4c04-c4e6-d4b7d634ed2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - Eleições</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " Nas eleições presidenciais americanas de 2016, a maioria das predições apontavam para a vitória de Hillary Clinton.Entretanto, a história nos mostrou o resultado oposto, e Donald Trump foi o presidente nos últimos 4 anos.Desta vez, os estatísticos reexaminaram seus modelos, para aumentar o grau de confiabilidade nos seus resultados.Nesta tentativa de otimização das predições, a inteligência artificial certamente não ficou de fora.<mark>O modelo desenvolvido pelo Dr. Hernan Makse, físico estatístico da Universidade da Cidade de Nova York, baseou suas predições em uma rede neural treinada para processar os sentimentos expressos nas redes sociais.</mark>O algoritmo fez a análise de cerca de 1 bilhão de tweets para chegar a uma estimativa dos resultados do pleito.No dia da eleição, 3 de novembro, o modelo estava indicando a vitória de Joe Biden.O Dr. Makse disse que seu trabalho começou já na eleição de 2016, e foi testado novamente nas eleições na Argentina ano passado.<mark>Desta vez, o modelo está treinando com cerca de 5 vezes mais dados que nas eleições americanas anteriores.</mark><mark>O trabalho não depende apenas da coleta dos dados, mas também de um tratamento estatístico adequado para levar em consideração duas variáveis externas: o viés de amostragem e a taxa de comparecimento.</mark>O primeiro fator se refere ao fato de que as redes sociais não necessariamente representam a população americana.<mark>A participação em redes sociais costuma ser maior nas cidades grandes, que de fato têm preferência por um dos candidatos, e o modelo deve ser corrigido para levar em consideração também a opinião das pessoas que não são ativas neste ambiente virtual.</mark>O segundo fator se deve à não-obrigatoriedade de votação nos Estados Unidos, ou seja, por mais que uma pessoa tenha sua preferência, pode ser que ela não compareça aos locais de votação para efetivá-la.Segundo o Dr. Makse, integrar estas duas variáveis em seu modelo é a parte mais importante do trabalho.Ele acredita ser esta uma das razões para que as estimativas da última eleição, baseadas em métodos tradicionais de coleta de informação, terem falhado.Sua equipe acompanhou as tendências apresentadas nas últimas eleições na Europa, e os modelos estão se revelando cada vez melhores.<mark>Quando seu modelo foi usado para predizer os resultados da eleição corrente usando dados brutos, Joe Biden apareceu como vencedor com larga vantagem.</mark>Após aplicar os mecanismos de correção para os dois vieses identificados, a vantagem diminuiu, mas Biden ainda é indicado como favorito.Parece que, desta vez, os algoritmos estão de fato contribuindo para que as predições sejam mais precisas. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_resumo('Eleições', s.sentences, best_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FuLiqlkqP-V"
   },
   "source": [
    "## Sumarização de mais textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DjnIZwpNe85B"
   },
   "outputs": [],
   "source": [
    "lista_artigos = ['https://iaexpert.academy/2020/11/06/ia-detecta-deep-fakes-produzidos-com-tecnicas-recentes/',\n",
    "                 'https://iaexpert.academy/2020/11/13/facebook-apresenta-novo-algoritmo-deteccao-fake-news/',\n",
    "                 'https://iaexpert.academy/2020/11/16/automl-aspectos-aplicacoes/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "woglfEn5fLLk",
    "outputId": "768f8ad8-5d0b-4bc4-8ebc-148cef8a096b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - IA detecta deep fakes produzidos com as técnicas mais recentes</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " <mark>No ano passado, pesquisadores da Universidade de Stanford publicaram um trabalho descrevendo uma tecnologia para sincronização de lábios que permitia que editores de vídeo pudessem alterar as palavras de quem está falando de forma quase imperceptível, o que permitia inserir ou remover palavras mesmo no meio de frases.</mark>A tecnologia tem aplicações benéficas, como durante a edição de filmes, onde não seria necessário refilmar sequências ou fazer malabarismos de montagem para que o resultado final ficasse de acordo com a vontade do diretor.Entretanto, ela também poderia ser utilizada com más intenções, como para produzir deep fakes com conteúdo desinformativo.Este ano, os mesmos pesquisadores apresentaram um novo trabalho capaz de detectar as alterações realizadas.<mark>Ele se baseia nas mesmas ideias desenvolvidas no trabalho anterior, que buscavam parear fonemas com o que eles chamaram de “visemas”, correspondentes à posição dos lábios durante a produção do fonema.</mark><mark>Apesar de as edições produzidas terem passado despercebidas por avaliadores humanos, ainda assim alguns artefatos são inseridos, sobretudo em fonemas com visemas bem característicos, como aqueles das letras M, B e P. Nestes casos, os lábios devem se fechar de forma firme.</mark>Pequenas diferenças entre as representações reais e criadas destes visemas foram utilizadas para detectar as sessões editadas dos vídeos com o uso de uma rede neural convolucional.No trabalho, os cientistas avaliaram vários vídeos adulterados do ex-presidente Barack Obama.O algoritmo, treinado com instâncias deste dataset, conseguiu identificar os casos de deep fake com precisão superior a 90%.Quando avaliado em um dataset mais amplo contendo outras pessoas, a precisão foi de 81%.<mark>Os autores comentam que detecções de deep fake com manipulação espacial e temporal limitada, como as edições feitas para alterar sutilmente um discurso de forma a mudar sua interpretação, são particularmente difíceis de detectar.</mark>Em casos de alto risco, a avaliação pode ser feita por uma pessoa qualificada, mas num ambiente mais amplo, técnicas automáticas são necessárias.<mark>Neste sentido, seu trabalho é um avanço importante no combate às más práticas associadas a esta tecnologia, já que apresenta resultados promissores avaliando deep fakes criados com as técnicas mais recentes.</mark> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - Facebook apresenta novo algoritmo para detecção de fake news</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " Já é de amplo reconhecimento que as redes sociais afetaram o panorama político nos países com eleições.<mark>Se por um lado, as plataformas permitem a disseminação de ideias e programas políticos, permitindo uma maior divulgação de candidatos menos populares, por outro, agentes mal-intencionados têm usado as redes para disseminar fake news e assim manipular as eleições em seu favor.</mark>Em função de todo este poder, nos últimos anos tem aumentado a pressão para que empresas como Facebook e Twitter tomem medidas de forma a amenizar os efeitos nocivos que suas plataformas podem ter.Em agosto deste ano, o Facebook apresentou na Conferência de Data Mining e Descoberta de Conhecimento ( ) um trabalho detalhando um modelo implementado pela empresa para, como diz o título do artigo, “melhorar a integridade da rede social”.<mark>O modelo faz uso de uma nova abstração chamada de ( ), que foi desenvolvida com o objetivo de reconhecer interações sociais “rebeldes” para que elas possam ser encaminhadas para tratamento individualizado.</mark>O modelo tem a estrutura de uma rede neural, treinado de forma supervisionada, e incorpora características estáticas e dinâmicas das chamadas entidades sociais.<mark>Isso quer dizer que, ao contrários dos métodos anteriores que focavam em apenas um desses aspectos – por exemplo, o histórico de postagem ou a lista de amigos das contas -, o novo algoritmo leva ambos em consideração, “desmontando” as interações em partes como quem esteve envolvido, e o que e onde aconteceu.</mark><mark>Isso foi possível graças à evolução recente nos domínios de embeddings de grafos, usados para representar interações em uma rede, e de aprendizagem de comportamento sequencial profunda, capaz de classificar a intenção de um agente através da consistência de seu comportamento.</mark>O resultado é uma compreensão mais detalhada sobre a natureza da conta e das postagens, que permite um controle mais fino dos moderadores.No artigo, o Facebook demonstrou o uso da ferramenta na prevenção da disseminação de fake news e na detecção de contas falsas.A abordagem certamente tem suas limitações.Em primeiro lugar, por se tratar de um problema supervisionado, instâncias devem ser rotuladas por pessoal técnico antes do treinamento, e nem sempre é fácil classificar um comportamento como malicioso.Em segundo, a ferramenta apenas identifica situações suspeitas, mas o plano de ação continua sendo delegado a um moderador, dependendo de avaliações de natureza bastante subjetiva.Mas há de se reconhecer os esforços da empresa em assumir responsabilidade pelo uso indevido da sua plataforma.<mark>Além de controlar o fluxo de informações falsas, as empresas pretendem manter a confiança em suas plataformas, para garantir que o ambiente mantenha sua percepção como saudável, construtivo e convidativo aos usuários.</mark> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - AutoML: aspectos e aplicações</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " Os algoritmos de machine learning são métodos numéricos implementados na forma de linguagem de programação, para que sejam processados por computadores.<mark>Estes métodos são formulados de forma que o algoritmo resolva um problema não através da sua modelagem matemática, que exigiria o conhecimento de uma equação específica que descreve o problema, mas sim através de um modelo mais genérico, capaz de aproximar a equação desconhecida, e que por um processo iterativo tende a convergir para uma solução.</mark>Seu grande sucesso se deve ao fato de que os computadores são ideais para implementar estas iterações, já que elas são blocos de operações matemáticas mais simples que se repetem até que a solução seja alcançada.Através das iterações, os algoritmos ajustam seus parâmetros internos de forma que o modelo seja capaz de reproduzir as respostas do problema a partir dos dados de entrada.É assim que os modelos de machine learning ganham a capacidade de fazer predições em novos dados.Apesar de o processo descrito acima ser automatizado, existem alguns parâmetros que, via de regra, não podem ser ajustados de maneira automática.Isto se deve principalmente ao fato de que não existe uma relação matemática precisa entre estes parâmetros e o desempenho do modelo, para que eles possam ser ajustados computacionalmente na direção da melhor solução.Além disso, eles costumam ser independentes entre si, de forma que diferentes combinações de valores de hiperparâmetros produzem resultados sem relação entre si, e pode ser que a solução ideal resida num espaço bastante distante de outras soluções adequadas.Os hiperparâmetros, que incluem a escolha do modelo e seus ajustes específicos, costumam ser configurados manualmente pelo desenvolvedor, que só vai saber sua qualidade ao final do processo de treinamento, avaliando as métricas de desempenho.<mark>O sucesso em sua escolha vai depender do conhecimento que o desenvolvedor tem sobre os dados com que está trabalhando, sua experiência em utilizar e ajustar os diferentes algoritmos disponíveis, e ainda uma boa dose de sorte.</mark>Mais recentemente, surgiram algumas bibliotecas buscando automatizar esta etapa inerentemente exploratória do processo de desenvolvimento de algoritmos de machine learning.Elas funcionam basicamente testando diferentes valores de hiperparâmetros e suas combinações, orientados por alguma heurística que estabelece uma direção para esta busca, retornando o desempenho do modelo nestas diferentes situações.Se por um lado esta abordagem ignora a intuição do desenvolvedor na escolha dos hiperparâmetros a serem testados, por outro ela poupa tempo que o desenvolvedor gastaria escrevendo código, e agora pode dispender em outras tarefas.A bem da verdade, há vantagens e desvantagens na sua utilização, além de fases específicas de um projeto onde elas podem ser de grande utilidade.Quando um cientista de dados inicia um novo projeto, é essencial desenvolver rapidamente uma ideia sobre sua viabilidade, já que, em uma empresa, o principal objetivo deste profissional é entregar valor.<mark>Como a natureza dos dados começa geralmente desconhecida, usar ferramentas de machine learning automático nesta fase inicial ajuda a estabelecer uma ideia mínima sobre o custo de oportunidade do projeto.</mark>Os modelos gerados desta forma podem servir como baseline, estabelecendo o desempenho mínimo possível a partir do qual a empresa pode decidir se a ideia toda vale a pena, se é sensato investir mais tempo e recursos para tentar melhorar este panorama inicial.O AutoML nesta fase inicial também ajuda a definir um cenário geral a ser explorado.Digamos que a abordagem revele que os melhores modelos são de uma determinada família de métodos numéricos.Então, é possível que os processos de otimização seguintes sejam melhor sucedidos se continuarem a exploração neste sentido.Assim, o cientista de dados já pula uma fase longa do desenvolvimento, podendo focar sua atenção na compreensão das tarefas mais específicas que o método de AutoML não é capaz de realizar.Outra grande vantagem de recorrer a métodos de AutoML é quando a empresa está em transição para ser um negócio orientado por dados.Nesta fase, ela ainda pode estar reticente sobre desenvolver toda uma nova área, estabelecer toda uma nova cultura, e o AutoML fornece uma indicação para responder se é hora de tomar essa decisão.Em muitas situações, faz mais sentido reciclar um desenvolvedor interno da empresa para fazer este estudo inicial do que contratar um profissional específico, com conhecimento técnico suficiente, para obter a mesma resposta.Diante da possibilidade de otimizar modelos usando ferramentas automáticas, muitos se perguntam se ainda há espaço para os cientistas de dados que faziam este trabalho manualmente.Mas certamente há.<mark>O AutoML ainda é incapaz de incorporar conhecimento que não seja facilmente expresso em operações matemáticas simples; ele não faz, por exemplo, as etapas de e análise causal, não resolve situações de , não considera como a escolha do modelo impacta a fase de produção do projeto, e não incorpora aspectos relacionados ao negócio.</mark>Pode ser que o AutoML encontre uma solução inviável em termos práticos.É papel do cientista de dados refinar seus resultados.<mark>De fato, a menor parte do tempo de um cientista de dados é gasta escrevendo código e desenvolvendo modelos; sua expertise está muito mais relacionada em como conectar o conhecimento oriundo dos dados com suas aplicações no mundo real.</mark>Ao invés de um concorrente, o AutoML é mais uma ferramenta que o cientista de dados dispõe, que automatiza uma parte mais braçal de seu trabalho para que ele possa se concentrar nas tarefas mais complexas.Alguns de seus pontos fortes disponíveis ao cientista de dados são: geração de baselines confiáveis e consistentes, aumento da reprodutibilidade e da robustez metodológica, disponibilidade de várias heurísticas de treinamento, e maior possibilidade de experimentação.A seguir apresentamos algumas das bibliotecas mais citadas atualmente.Fazendo uso da extensa coleção de métodos do Scikit-Learn, temos a e a , que fazem uma busca no espaço de soluções usando otimização bayesiana.A se baseia em um pipeline que busca valores de hiperparâmetros usando uma abordagem baseada em árvore de decisão.A permite a otimização de redes neurais construídas usando o Keras.A pesquisa os melhores parâmetros para modelos baseados em árvores de decisão e redes neurais.Via de regra, estas bibliotecas são extremamente fáceis de implementar.A ideia geral é fornecer os dados e alguns limitantes como o tempo disponível para rodar a otimização.Muito prático, não?Assim como várias tarefas de outras profissões vão sendo substituídas por métodos automatizados, algumas atribuições do cientista de dados também são passíveis de passar por esta transformação.Mas a ciência de dados tem muito a ganhar com o uso das ferramentas de AutoML, explorando seus pontos fortes e deixando os profissionais livres para trabalharem em seus pontos fracos.No final do dia, o AutoML é uma ferramenta poderosa disponível no arsenal do cientista de dados, mas ainda assim apenas uma ferramenta, que precisa de um operador para coordenar como esta pequena parte do trabalho vai integrar com o todo. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for url in lista_artigos:\n",
    "  #print(url)\n",
    "  g = Goose()\n",
    "  artigo = g.extract(url)\n",
    "  best_sentences = s.summarize(artigo.cleaned_text)\n",
    "  visualize_resumo(artigo.title, s.sentences, best_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AGyBRVNwwXdJ",
    "outputId": "eb91c457-5b6c-4507-c671-9a0a6d1fbf01"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - IA detecta deep fakes produzidos com as técnicas mais recentes</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " <mark>No ano passado, pesquisadores da Universidade de Stanford publicaram um trabalho descrevendo uma tecnologia para sincronização de lábios que permitia que editores de vídeo pudessem alterar as palavras de quem está falando de forma quase imperceptível, o que permitia inserir ou remover palavras mesmo no meio de frases.</mark><mark>A tecnologia tem aplicações benéficas, como durante a edição de filmes, onde não seria necessário refilmar sequências ou fazer malabarismos de montagem para que o resultado final ficasse de acordo com a vontade do diretor.</mark>Entretanto, ela também poderia ser utilizada com más intenções, como para produzir deep fakes com conteúdo desinformativo.Este ano, os mesmos pesquisadores apresentaram um novo trabalho capaz de detectar as alterações realizadas.<mark>Ele se baseia nas mesmas ideias desenvolvidas no trabalho anterior, que buscavam parear fonemas com o que eles chamaram de “visemas”, correspondentes à posição dos lábios durante a produção do fonema.</mark>Apesar de as edições produzidas terem passado despercebidas por avaliadores humanos, ainda assim alguns artefatos são inseridos, sobretudo em fonemas com visemas bem característicos, como aqueles das letras M, B e P. Nestes casos, os lábios devem se fechar de forma firme.Pequenas diferenças entre as representações reais e criadas destes visemas foram utilizadas para detectar as sessões editadas dos vídeos com o uso de uma rede neural convolucional.No trabalho, os cientistas avaliaram vários vídeos adulterados do ex-presidente Barack Obama.O algoritmo, treinado com instâncias deste dataset, conseguiu identificar os casos de deep fake com precisão superior a 90%.Quando avaliado em um dataset mais amplo contendo outras pessoas, a precisão foi de 81%.<mark>Os autores comentam que detecções de deep fake com manipulação espacial e temporal limitada, como as edições feitas para alterar sutilmente um discurso de forma a mudar sua interpretação, são particularmente difíceis de detectar.</mark>Em casos de alto risco, a avaliação pode ser feita por uma pessoa qualificada, mas num ambiente mais amplo, técnicas automáticas são necessárias.<mark>Neste sentido, seu trabalho é um avanço importante no combate às más práticas associadas a esta tecnologia, já que apresenta resultados promissores avaliando deep fakes criados com as técnicas mais recentes.</mark> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - Facebook apresenta novo algoritmo para detecção de fake news</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " Já é de amplo reconhecimento que as redes sociais afetaram o panorama político nos países com eleições.Se por um lado, as plataformas permitem a disseminação de ideias e programas políticos, permitindo uma maior divulgação de candidatos menos populares, por outro, agentes mal-intencionados têm usado as redes para disseminar fake news e assim manipular as eleições em seu favor.Em função de todo este poder, nos últimos anos tem aumentado a pressão para que empresas como Facebook e Twitter tomem medidas de forma a amenizar os efeitos nocivos que suas plataformas podem ter.<mark>Em agosto deste ano, o Facebook apresentou na Conferência de Data Mining e Descoberta de Conhecimento ( ) um trabalho detalhando um modelo implementado pela empresa para, como diz o título do artigo, “melhorar a integridade da rede social”.</mark><mark>O modelo faz uso de uma nova abstração chamada de ( ), que foi desenvolvida com o objetivo de reconhecer interações sociais “rebeldes” para que elas possam ser encaminhadas para tratamento individualizado.</mark>O modelo tem a estrutura de uma rede neural, treinado de forma supervisionada, e incorpora características estáticas e dinâmicas das chamadas entidades sociais.Isso quer dizer que, ao contrários dos métodos anteriores que focavam em apenas um desses aspectos – por exemplo, o histórico de postagem ou a lista de amigos das contas -, o novo algoritmo leva ambos em consideração, “desmontando” as interações em partes como quem esteve envolvido, e o que e onde aconteceu.<mark>Isso foi possível graças à evolução recente nos domínios de embeddings de grafos, usados para representar interações em uma rede, e de aprendizagem de comportamento sequencial profunda, capaz de classificar a intenção de um agente através da consistência de seu comportamento.</mark>O resultado é uma compreensão mais detalhada sobre a natureza da conta e das postagens, que permite um controle mais fino dos moderadores.<mark>No artigo, o Facebook demonstrou o uso da ferramenta na prevenção da disseminação de fake news e na detecção de contas falsas.</mark>A abordagem certamente tem suas limitações.Em primeiro lugar, por se tratar de um problema supervisionado, instâncias devem ser rotuladas por pessoal técnico antes do treinamento, e nem sempre é fácil classificar um comportamento como malicioso.Em segundo, a ferramenta apenas identifica situações suspeitas, mas o plano de ação continua sendo delegado a um moderador, dependendo de avaliações de natureza bastante subjetiva.<mark>Mas há de se reconhecer os esforços da empresa em assumir responsabilidade pelo uso indevido da sua plataforma.</mark>Além de controlar o fluxo de informações falsas, as empresas pretendem manter a confiança em suas plataformas, para garantir que o ambiente mantenha sua percepção como saudável, construtivo e convidativo aos usuários. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Resumo do texto - AutoML: aspectos e aplicações</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " Os algoritmos de machine learning são métodos numéricos implementados na forma de linguagem de programação, para que sejam processados por computadores.<mark>Estes métodos são formulados de forma que o algoritmo resolva um problema não através da sua modelagem matemática, que exigiria o conhecimento de uma equação específica que descreve o problema, mas sim através de um modelo mais genérico, capaz de aproximar a equação desconhecida, e que por um processo iterativo tende a convergir para uma solução.</mark>Seu grande sucesso se deve ao fato de que os computadores são ideais para implementar estas iterações, já que elas são blocos de operações matemáticas mais simples que se repetem até que a solução seja alcançada.Através das iterações, os algoritmos ajustam seus parâmetros internos de forma que o modelo seja capaz de reproduzir as respostas do problema a partir dos dados de entrada.É assim que os modelos de machine learning ganham a capacidade de fazer predições em novos dados.Apesar de o processo descrito acima ser automatizado, existem alguns parâmetros que, via de regra, não podem ser ajustados de maneira automática.<mark>Isto se deve principalmente ao fato de que não existe uma relação matemática precisa entre estes parâmetros e o desempenho do modelo, para que eles possam ser ajustados computacionalmente na direção da melhor solução.</mark>Além disso, eles costumam ser independentes entre si, de forma que diferentes combinações de valores de hiperparâmetros produzem resultados sem relação entre si, e pode ser que a solução ideal resida num espaço bastante distante de outras soluções adequadas.<mark>Os hiperparâmetros, que incluem a escolha do modelo e seus ajustes específicos, costumam ser configurados manualmente pelo desenvolvedor, que só vai saber sua qualidade ao final do processo de treinamento, avaliando as métricas de desempenho.</mark>O sucesso em sua escolha vai depender do conhecimento que o desenvolvedor tem sobre os dados com que está trabalhando, sua experiência em utilizar e ajustar os diferentes algoritmos disponíveis, e ainda uma boa dose de sorte.Mais recentemente, surgiram algumas bibliotecas buscando automatizar esta etapa inerentemente exploratória do processo de desenvolvimento de algoritmos de machine learning.Elas funcionam basicamente testando diferentes valores de hiperparâmetros e suas combinações, orientados por alguma heurística que estabelece uma direção para esta busca, retornando o desempenho do modelo nestas diferentes situações.Se por um lado esta abordagem ignora a intuição do desenvolvedor na escolha dos hiperparâmetros a serem testados, por outro ela poupa tempo que o desenvolvedor gastaria escrevendo código, e agora pode dispender em outras tarefas.A bem da verdade, há vantagens e desvantagens na sua utilização, além de fases específicas de um projeto onde elas podem ser de grande utilidade.Quando um cientista de dados inicia um novo projeto, é essencial desenvolver rapidamente uma ideia sobre sua viabilidade, já que, em uma empresa, o principal objetivo deste profissional é entregar valor.Como a natureza dos dados começa geralmente desconhecida, usar ferramentas de machine learning automático nesta fase inicial ajuda a estabelecer uma ideia mínima sobre o custo de oportunidade do projeto.Os modelos gerados desta forma podem servir como baseline, estabelecendo o desempenho mínimo possível a partir do qual a empresa pode decidir se a ideia toda vale a pena, se é sensato investir mais tempo e recursos para tentar melhorar este panorama inicial.O AutoML nesta fase inicial também ajuda a definir um cenário geral a ser explorado.Digamos que a abordagem revele que os melhores modelos são de uma determinada família de métodos numéricos.Então, é possível que os processos de otimização seguintes sejam melhor sucedidos se continuarem a exploração neste sentido.Assim, o cientista de dados já pula uma fase longa do desenvolvimento, podendo focar sua atenção na compreensão das tarefas mais específicas que o método de AutoML não é capaz de realizar.Outra grande vantagem de recorrer a métodos de AutoML é quando a empresa está em transição para ser um negócio orientado por dados.Nesta fase, ela ainda pode estar reticente sobre desenvolver toda uma nova área, estabelecer toda uma nova cultura, e o AutoML fornece uma indicação para responder se é hora de tomar essa decisão.<mark>Em muitas situações, faz mais sentido reciclar um desenvolvedor interno da empresa para fazer este estudo inicial do que contratar um profissional específico, com conhecimento técnico suficiente, para obter a mesma resposta.</mark>Diante da possibilidade de otimizar modelos usando ferramentas automáticas, muitos se perguntam se ainda há espaço para os cientistas de dados que faziam este trabalho manualmente.Mas certamente há.<mark>O AutoML ainda é incapaz de incorporar conhecimento que não seja facilmente expresso em operações matemáticas simples; ele não faz, por exemplo, as etapas de e análise causal, não resolve situações de , não considera como a escolha do modelo impacta a fase de produção do projeto, e não incorpora aspectos relacionados ao negócio.</mark>Pode ser que o AutoML encontre uma solução inviável em termos práticos.É papel do cientista de dados refinar seus resultados.De fato, a menor parte do tempo de um cientista de dados é gasta escrevendo código e desenvolvendo modelos; sua expertise está muito mais relacionada em como conectar o conhecimento oriundo dos dados com suas aplicações no mundo real.Ao invés de um concorrente, o AutoML é mais uma ferramenta que o cientista de dados dispõe, que automatiza uma parte mais braçal de seu trabalho para que ele possa se concentrar nas tarefas mais complexas.Alguns de seus pontos fortes disponíveis ao cientista de dados são: geração de baselines confiáveis e consistentes, aumento da reprodutibilidade e da robustez metodológica, disponibilidade de várias heurísticas de treinamento, e maior possibilidade de experimentação.A seguir apresentamos algumas das bibliotecas mais citadas atualmente.Fazendo uso da extensa coleção de métodos do Scikit-Learn, temos a e a , que fazem uma busca no espaço de soluções usando otimização bayesiana.A se baseia em um pipeline que busca valores de hiperparâmetros usando uma abordagem baseada em árvore de decisão.A permite a otimização de redes neurais construídas usando o Keras.A pesquisa os melhores parâmetros para modelos baseados em árvores de decisão e redes neurais.Via de regra, estas bibliotecas são extremamente fáceis de implementar.A ideia geral é fornecer os dados e alguns limitantes como o tempo disponível para rodar a otimização.Muito prático, não?Assim como várias tarefas de outras profissões vão sendo substituídas por métodos automatizados, algumas atribuições do cientista de dados também são passíveis de passar por esta transformação.Mas a ciência de dados tem muito a ganhar com o uso das ferramentas de AutoML, explorando seus pontos fortes e deixando os profissionais livres para trabalharem em seus pontos fracos.No final do dia, o AutoML é uma ferramenta poderosa disponível no arsenal do cientista de dados, mas ainda assim apenas uma ferramenta, que precisa de um operador para coordenar como esta pequena parte do trabalho vai integrar com o todo. "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s2 = WordFreqSummarizer(n_sentences=5, lemma=True, spacy_model=\"pt_core_news_lg\")\n",
    "\n",
    "for url in lista_artigos:\n",
    "  #print(url)\n",
    "  g = Goose()\n",
    "  artigo = g.extract(url)\n",
    "  melhores_sentencas = s2.summarize(artigo.cleaned_text)\n",
    "  visualize_resumo(artigo.title, s2.sentences, melhores_sentencas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
